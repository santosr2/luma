--- Lexer tests for Luma
describe("Lexer", function()
    local lexer = require("luma.lexer")
    local tokens = require("luma.lexer.tokens")
    local T = tokens.types

    describe("native syntax", function()
        describe("text", function()
            it("tokenizes plain text", function()
                local result = lexer.tokenize("Hello, World!")
                assert.equals(2, #result)
                assert.equals(T.TEXT, result[1].type)
                assert.equals("Hello, World!", result[1].value)
                assert.equals(T.EOF, result[2].type)
            end)

            it("handles empty input", function()
                local result = lexer.tokenize("")
                assert.equals(1, #result)
                assert.equals(T.EOF, result[1].type)
            end)

            it("preserves newlines in text", function()
                local result = lexer.tokenize("line1\nline2\nline3")
                assert.equals(2, #result)
                assert.equals(T.TEXT, result[1].type)
                assert.equals("line1\nline2\nline3", result[1].value)
            end)
        end)

        describe("simple interpolation", function()
            it("tokenizes $var", function()
                local result = lexer.tokenize("Hello, $name!")
                assert.equals(4, #result)
                assert.equals(T.TEXT, result[1].type)
                assert.equals("Hello, ", result[1].value)
                assert.equals(T.INTERP_SIMPLE, result[2].type)
                assert.equals("name", result[2].value)
                assert.equals(T.TEXT, result[3].type)
                assert.equals("!", result[3].value)
            end)

            it("tokenizes $foo.bar.baz", function()
                local result = lexer.tokenize("$user.profile.name")
                assert.equals(2, #result)
                assert.equals(T.INTERP_SIMPLE, result[1].type)
                assert.equals("user.profile.name", result[1].value)
            end)

            it("escapes $$ to literal $", function()
                local result = lexer.tokenize("Price: $$100")
                assert.equals(2, #result)
                assert.equals(T.TEXT, result[1].type)
                assert.equals("Price: $100", result[1].value)
            end)
        end)

        describe("expression interpolation", function()
            it("tokenizes ${expr}", function()
                local result = lexer.tokenize("${name}")
                assert.equals(4, #result)
                assert.equals(T.INTERP_START, result[1].type)
                assert.equals(T.IDENT, result[2].type)
                assert.equals("name", result[2].value)
                assert.equals(T.INTERP_END, result[3].type)
            end)

            it("tokenizes expressions with operators", function()
                local result = lexer.tokenize("${1 + 2}")
                assert.equals(6, #result)
                assert.equals(T.INTERP_START, result[1].type)
                assert.equals(T.NUMBER, result[2].type)
                assert.equals(1, result[2].value)
                assert.equals(T.PLUS, result[3].type)
                assert.equals(T.NUMBER, result[4].type)
                assert.equals(2, result[4].value)
                assert.equals(T.INTERP_END, result[5].type)
            end)

            it("tokenizes filter expressions", function()
                local result = lexer.tokenize("${name | upper}")
                assert.equals(6, #result)
                assert.equals(T.INTERP_START, result[1].type)
                assert.equals(T.IDENT, result[2].type)
                assert.equals(T.PIPE, result[3].type)
                assert.equals(T.IDENT, result[4].type)
                assert.equals("upper", result[4].value)
                assert.equals(T.INTERP_END, result[5].type)
            end)
        end)

        describe("directives", function()
            it("tokenizes @if", function()
                local result = lexer.tokenize("@if condition\ntext\n@end")
                assert.equals(T.DIR_IF, result[1].type)
                assert.equals(T.IDENT, result[2].type)
                assert.equals("condition", result[2].value)
            end)

            it("tokenizes @for with in", function()
                local result = lexer.tokenize("@for item in items\n@end")
                assert.equals(T.DIR_FOR, result[1].type)
                assert.equals(T.IDENT, result[2].type)
                assert.equals("item", result[2].value)
                assert.equals(T.IN, result[3].type)
                assert.equals(T.IDENT, result[4].type)
                assert.equals("items", result[4].value)
            end)

            it("tokenizes @let", function()
                local result = lexer.tokenize("@let x = 10\n")
                assert.equals(T.DIR_LET, result[1].type)
                assert.equals(T.IDENT, result[2].type)
                assert.equals("x", result[2].value)
                assert.equals(T.ASSIGN, result[3].type)
                assert.equals(T.NUMBER, result[4].type)
                assert.equals(10, result[4].value)
            end)

            it("tokenizes @end", function()
                local result = lexer.tokenize("@end")
                assert.equals(2, #result)
                assert.equals(T.DIR_END, result[1].type)
            end)

            it("tokenizes @else", function()
                local result = lexer.tokenize("@else")
                assert.equals(2, #result)
                assert.equals(T.DIR_ELSE, result[1].type)
            end)

            it("tokenizes @elif", function()
                local result = lexer.tokenize("@elif condition")
                assert.equals(T.DIR_ELIF, result[1].type)
                assert.equals(T.IDENT, result[2].type)
            end)

            it("tokenizes @# comment", function()
                local result = lexer.tokenize("@# this is a comment")
                assert.equals(2, #result)
                assert.equals(T.DIR_COMMENT, result[1].type)
                assert.equals(" this is a comment", result[1].value)
            end)
        end)

        describe("literals", function()
            it("tokenizes string literals", function()
                local result = lexer.tokenize('${"hello"}')
                assert.equals(T.STRING, result[2].type)
                assert.equals("hello", result[2].value)
            end)

            it("tokenizes number literals", function()
                local result = lexer.tokenize("${42}")
                assert.equals(T.NUMBER, result[2].type)
                assert.equals(42, result[2].value)
            end)

            it("tokenizes float literals", function()
                local result = lexer.tokenize("${3.14}")
                assert.equals(T.NUMBER, result[2].type)
                assert.equals(3.14, result[2].value)
            end)

            it("tokenizes boolean true", function()
                local result = lexer.tokenize("${true}")
                assert.equals(T.BOOLEAN, result[2].type)
                assert.equals(true, result[2].value)
            end)

            it("tokenizes boolean false", function()
                local result = lexer.tokenize("${false}")
                assert.equals(T.BOOLEAN, result[2].type)
                assert.equals(false, result[2].value)
            end)

            it("tokenizes nil", function()
                local result = lexer.tokenize("${nil}")
                assert.equals(T.NIL, result[2].type)
            end)
        end)

        describe("operators", function()
            it("tokenizes comparison operators", function()
                local result = lexer.tokenize("${a == b}")
                assert.equals(T.EQ, result[3].type)

                result = lexer.tokenize("${a != b}")
                assert.equals(T.NE, result[3].type)

                result = lexer.tokenize("${a < b}")
                assert.equals(T.LT, result[3].type)

                result = lexer.tokenize("${a <= b}")
                assert.equals(T.LE, result[3].type)

                result = lexer.tokenize("${a > b}")
                assert.equals(T.GT, result[3].type)

                result = lexer.tokenize("${a >= b}")
                assert.equals(T.GE, result[3].type)
            end)

            it("tokenizes logical operators", function()
                local result = lexer.tokenize("${a and b}")
                assert.equals(T.AND, result[3].type)

                result = lexer.tokenize("${a or b}")
                assert.equals(T.OR, result[3].type)

                result = lexer.tokenize("${not a}")
                assert.equals(T.NOT, result[2].type)
            end)

            it("tokenizes pipe operators", function()
                local result = lexer.tokenize("${a | filter}")
                assert.equals(T.PIPE, result[3].type)

                result = lexer.tokenize("${a |> filter()}")
                assert.equals(T.PIPE_ARROW, result[3].type)
            end)
        end)
    end)
end)
